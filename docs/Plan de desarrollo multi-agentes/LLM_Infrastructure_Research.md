# Investigaci√≥n de Infraestructura LLM - Securetag

**Fecha**: 2025-11-19  
**Agente**: Infra  
**Iteraci√≥n**: 2

## üìä Contexto

El proyecto Securetag utiliza actualmente el modelo `securetag-ai-agent:latest` (4.4 GB) corriendo en Ollama localmente. Este modelo se usa para an√°lisis de hallazgos de seguridad. Se requiere determinar la mejor estrategia de despliegue considerando el futuro deployment en DigitalOcean.

## üîç Opciones Investigadas

### Opci√≥n A: Docker Local (Ollama en Contenedor)

**Descripci√≥n**: Containerizar Ollama con el modelo dentro de Docker Compose.

**Requisitos T√©cnicos**:
- **GPU**: NVIDIA con compute capability 5.0+ (RTX 30/40 series recomendado)
- **Driver**: NVIDIA driver 531+
- **Software**: `nvidia-container-toolkit` para acceso GPU en Docker
- **RAM**: M√≠nimo 8 GB (16 GB recomendado para modelo de 4.4 GB)
- **Storage**: ~10 GB (modelo + overhead)
- **CPU**: Puede correr en CPU pero con latencia significativamente mayor

**Configuraci√≥n Docker Compose**:
```yaml
services:
  ollama:
    image: ollama/ollama:latest
    volumes:
      - ./data/ollama:/root/.ollama
    ports:
      - "11434:11434"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
```

**Pros**:
- ‚úÖ Control total del entorno
- ‚úÖ Sin costos adicionales de hosting
- ‚úÖ Latencia m√≠nima (local)
- ‚úÖ Privacidad total de datos
- ‚úÖ F√°cil integraci√≥n con stack actual
- ‚úÖ Ideal para desarrollo

**Contras**:
- ‚ùå Requiere GPU NVIDIA en m√°quina de desarrollo
- ‚ùå No escalable para producci√≥n multi-tenant
- ‚ùå Rendimiento limitado en CPU
- ‚ùå No disponible en DigitalOcean Droplets est√°ndar

---

### Opci√≥n B: DigitalOcean GPU Droplets

**Descripci√≥n**: Usar GPU Droplets de DigitalOcean para hospedar Ollama.

**Disponibilidad 2025**:
- **GPUs Disponibles**: RTX 4000 Ada, RTX 6000 Ada, L40S, H100, H200
- **Regiones**: NYC2, TOR1, ATL1, AMS3, EU
- **Estado**: General Availability desde Mayo 2025

**Pricing (estimado)**:
- **RTX A4000**: ~$0.76/hora (~$547/mes)
- **RTX 4000 Ada**: ~$1.00/hora (~$720/mes)
- **NVIDIA A100**: ~$3.09/hora (~$2,225/mes)
- **Billing**: Por segundo, m√≠nimo 5 minutos

**Requisitos**:
- **Setup**: Instalar Docker + nvidia-container-toolkit + Ollama
- **Networking**: Configurar firewall para acceso desde App/Worker
- **Storage**: Volumen persistente para modelos

**Pros**:
- ‚úÖ Infraestructura profesional en mismo proveedor
- ‚úÖ GPUs potentes (RTX 4000+, H100)
- ‚úÖ Escalabilidad vertical (cambiar GPU)
- ‚úÖ SLA y soporte de DigitalOcean
- ‚úÖ Misma red privada que App/Worker

**Contras**:
- ‚ùå Costo continuo (~$547-$2,225/mes)
- ‚ùå Billing corre aunque est√© idle
- ‚ùå Requiere gesti√≥n de servidor
- ‚ùå Overkill para desarrollo/testing

---

### Opci√≥n C: RunPod.io

**Descripci√≥n**: Plataforma especializada en hosting de GPU para ML/AI.

#### C1: RunPod Serverless

**Pricing**:
- **T4 GPU**: $0.40/hora (solo tiempo activo)
- **A100 80GB**: $2.17/hora (solo tiempo activo)
- **Billing**: Por segundo de procesamiento activo
- **Idle Cost**: $0 (escala a cero)

**Caracter√≠sticas**:
- FlashBoot: Cold start ~500ms
- Auto-scaling basado en demanda
- API REST para inferencia
- Sin costos de ingress/egress

**Pros**:
- ‚úÖ **Costo √≥ptimo**: Solo pagas por inferencia activa
- ‚úÖ Escala a cero cuando no se usa
- ‚úÖ Ideal para tr√°fico variable
- ‚úÖ Sin gesti√≥n de infraestructura
- ‚úÖ Cold start r√°pido (500ms)
- ‚úÖ Hasta 15% m√°s barato que competidores

**Contras**:
- ‚ùå Latencia adicional (cold start + red)
- ‚ùå Requiere integraci√≥n API (cambio en c√≥digo)
- ‚ùå Dependencia de servicio externo
- ‚ùå Menos control del entorno

#### C2: RunPod Pods (On-Demand/Spot)

**Pricing**:
- **A100 80GB On-Demand**: $1.99/hora (~$1,433/mes)
- **A100 80GB Spot**: $0.89/hora (~$641/mes)
- **H100 80GB On-Demand**: $4.49/hora (~$3,233/mes)
- **H100 80GB Spot**: $2.49/hora (~$1,793/mes)

**Caracter√≠sticas**:
- Pods dedicados con control total
- Spot: 55% descuento pero puede interrumpirse
- Savings Plans: 15-25% descuento con compromiso 3-6 meses

**Pros**:
- ‚úÖ M√°s barato que DigitalOcean
- ‚úÖ Spot Pods muy econ√≥micos
- ‚úÖ Control total del contenedor
- ‚úÖ Especializado en ML/AI

**Contras**:
- ‚ùå Spot Pods pueden interrumpirse
- ‚ùå Fuera de infraestructura DigitalOcean
- ‚ùå Latencia de red externa

---

## üìä Tabla Comparativa

| Criterio | Docker Local | DigitalOcean GPU | RunPod Serverless | RunPod Pods (Spot) |
|----------|--------------|------------------|-------------------|-------------------|
| **Costo Mensual** | $0 | $547-$2,225 | ~$50-$200* | ~$641-$1,793 |
| **GPU Requerida** | NVIDIA local | RTX 4000+, H100 | T4, A100 (managed) | A100, H100 |
| **RAM Recomendada** | 16 GB | Incluida | Incluida | Incluida |
| **Latencia** | <10ms | ~20-50ms | ~100-300ms | ~50-100ms |
| **Escalabilidad** | No | Vertical | Auto (horizontal) | Manual |
| **Idle Cost** | $0 | 100% | $0 | 100% |
| **Setup Complexity** | Media | Alta | Baja | Media |
| **Integraci√≥n** | Nativa | Nativa | API REST | Nativa |
| **Privacidad** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê |
| **Disponibilidad** | Depende dev | 99.9% SLA | 99.9% | Variable (Spot) |

\* *Estimado para uso intermitente (10-20 hrs/mes de inferencia)*

---

## üéØ Recomendaciones

### Para Desarrollo/Testing (AHORA)

**Recomendaci√≥n**: **Opci√≥n A - Docker Local**

**Justificaci√≥n**:
1. **Costo cero**: No hay gastos adicionales durante desarrollo
2. **Latencia m√≠nima**: Respuesta instant√°nea para debugging
3. **Control total**: F√°cil modificar modelo y configuraci√≥n
4. **Privacidad**: Datos sensibles no salen de la m√°quina
5. **Ya funciona**: El setup actual ya est√° operativo

**Implementaci√≥n**:
- Agregar servicio `ollama` a `docker-compose.yml`
- Documentar c√≥mo cargar modelo `securetag-ai-agent:latest`
- Configurar variables de entorno para App/Worker

**Requisito**: M√°quina de desarrollo con GPU NVIDIA (ya disponible seg√∫n `ollama list`)

---

### Para Producci√≥n (DESPLIEGUE EN DIGITALOCEAN)

**Recomendaci√≥n**: **Opci√≥n C1 - RunPod Serverless**

**Justificaci√≥n**:

1. **Costo-efectividad**: 
   - Solo pagas por tiempo de inferencia activo
   - Estimado: $50-$200/mes vs $547-$2,225/mes (DigitalOcean)
   - Ahorro: ~75-90% en costos

2. **Escalabilidad**:
   - Auto-scaling seg√∫n demanda
   - Perfecto para SaaS multi-tenant con tr√°fico variable
   - No requiere provisionar GPU para picos

3. **Simplicidad operacional**:
   - Sin gesti√≥n de servidores GPU
   - Sin preocupaci√≥n por actualizaciones de drivers
   - Infraestructura managed

4. **Patr√≥n de uso**:
   - An√°lisis de seguridad es intermitente (no 24/7)
   - Usuario espera ~5-30 segundos por an√°lisis
   - Cold start de 500ms es aceptable

**Trade-offs aceptables**:
- Latencia adicional: 100-300ms (aceptable para an√°lisis no real-time)
- Dependencia externa: Mitigado con SLA 99.9%
- Cambio de c√≥digo: M√≠nimo (cambiar de Ollama local a API REST)

---

### Alternativa para Producci√≥n (si se requiere control total)

**Recomendaci√≥n Secundaria**: **Opci√≥n C2 - RunPod Pods Spot**

**Cu√°ndo usar**:
- Si se requiere control total del entorno Ollama
- Si la latencia debe ser <100ms
- Si se prefiere arquitectura similar a desarrollo

**Ventajas sobre DigitalOcean**:
- 55% m√°s barato ($641 vs $1,433/mes para A100)
- Especializado en ML/AI
- Mejor precio/rendimiento

**Desventaja**:
- Pods Spot pueden interrumpirse (mitigable con auto-restart)

---

## üöÄ Plan de Implementaci√≥n Recomendado

### Fase 1: Desarrollo (Inmediato)
1. Agregar servicio `ollama` a `docker-compose.yml`
2. Configurar GPU access con `nvidia-container-toolkit`
3. Documentar proceso de carga de modelo
4. Actualizar variables de entorno en App/Worker

### Fase 2: Preparaci√≥n para Producci√≥n
1. Crear cuenta en RunPod.io
2. Subir modelo `securetag-ai-agent:latest` a RunPod
3. Configurar endpoint Serverless
4. Implementar cliente API REST en App/Worker
5. Testing de latencia y throughput

### Fase 3: Despliegue
1. Configurar variable de entorno `OLLAMA_HOST` para apuntar a RunPod
2. Desplegar App/Worker en DigitalOcean (sin GPU)
3. Monitorear costos y latencia
4. Ajustar seg√∫n m√©tricas reales

---

## üìà Proyecci√≥n de Costos (Producci√≥n)

### Escenario: 100 an√°lisis/d√≠a

**RunPod Serverless**:
- Tiempo por an√°lisis: ~30 segundos
- Total mensual: 100 √ó 30 √ó 30 = 25 horas
- Costo (A100): 25 hrs √ó $2.17 = **$54.25/mes**

**DigitalOcean GPU Droplet**:
- Costo fijo: 730 hrs √ó $3.09 = **$2,255.70/mes**

**Ahorro**: **$2,201.45/mes (97.6%)**

### Escenario: 1,000 an√°lisis/d√≠a

**RunPod Serverless**:
- Total mensual: 1,000 √ó 30 √ó 30 = 250 horas
- Costo (A100): 250 hrs √ó $2.17 = **$542.50/mes**

**DigitalOcean GPU Droplet**:
- Costo fijo: **$2,255.70/mes**

**Ahorro**: **$1,713.20/mes (76%)**

> [!NOTE]
> RunPod Serverless es m√°s econ√≥mico hasta ~1,040 an√°lisis/d√≠a. Para vol√∫menes mayores, considerar RunPod Pods On-Demand.

---

## ‚úÖ Conclusi√≥n

**Desarrollo**: Docker Local (Opci√≥n A)  
**Producci√≥n**: RunPod Serverless (Opci√≥n C1)

Esta estrategia dual optimiza:
- **Costo**: $0 en desarrollo, ~$50-$500/mes en producci√≥n
- **Experiencia de desarrollo**: Latencia m√≠nima, control total
- **Escalabilidad**: Auto-scaling en producci√≥n
- **Simplicidad**: Infraestructura managed en producci√≥n

**Pr√≥ximos pasos**: Implementar Fase 1 (Docker Local) para continuar desarrollo.
