# Documento de Evidencia - Infra

**Agente**: Infra  
**Iteraci√≥n**: 2  
**Fecha**: 2025-11-19 16:55  
**Estatus**: Completado

## üìã Reporte T√©cnico

Se complet√≥ la investigaci√≥n e implementaci√≥n de infraestructura LLM para el proyecto Securetag, evaluando tres opciones de despliegue y implementando la soluci√≥n recomendada para desarrollo.

### Archivos Modificados
*   `docker-compose.yml`: Agregado servicio `ollama` con soporte GPU
*   `docs/LLM_Infrastructure_Research.md`: Documento de investigaci√≥n completo
*   `docs/Ollama_Setup_Guide.md`: Gu√≠a de setup y troubleshooting

### Infraestructura Implementada

#### Servicio Ollama
- **Imagen**: `ollama/ollama:latest`
- **Puerto**: 11434
- **GPU**: Configurado con `nvidia-container-toolkit`
- **Volumen**: `./data/ollama` para persistencia de modelos
- **Red**: `securetag-net` (integrado con App/Worker)
- **Health Check**: Verifica disponibilidad cada 30s

#### Variables de Entorno
Agregadas a `securetag-app` y `securetag-worker`:
```yaml
OLLAMA_HOST: http://ollama:11434
```

### Investigaci√≥n Realizada

#### Opci√≥n A: Docker Local (Ollama en Contenedor) ‚úÖ IMPLEMENTADA
- **Costo**: $0
- **Latencia**: <10ms
- **Pros**: Control total, sin costos, ideal para desarrollo
- **Contras**: Requiere GPU NVIDIA, no escalable para producci√≥n

#### Opci√≥n B: DigitalOcean GPU Droplets
- **Costo**: $547-$2,225/mes
- **GPUs**: RTX 4000 Ada, RTX 6000 Ada, L40S, H100
- **Pros**: Misma infraestructura, GPUs potentes
- **Contras**: Costo continuo alto, billing 24/7

#### Opci√≥n C: RunPod.io ‚≠ê RECOMENDADA PARA PRODUCCI√ìN
- **Serverless**: $0.40-$2.17/hora (solo tiempo activo)
- **Pods Spot**: $0.89-$2.49/hora (55% descuento)
- **Pros**: Ahorro 76-97% vs DigitalOcean, auto-scaling
- **Contras**: Latencia adicional, dependencia externa

### An√°lisis Comparativo

| Criterio | Docker Local | DigitalOcean | RunPod Serverless |
|----------|--------------|--------------|-------------------|
| Costo/mes | $0 | $547-$2,225 | $50-$200* |
| Latencia | <10ms | ~20-50ms | ~100-300ms |
| Escalabilidad | No | Vertical | Auto |
| Idle Cost | $0 | 100% | $0 |

\* *Para 100-1,000 an√°lisis/d√≠a*

### Recomendaciones

**Desarrollo**: Docker Local (Opci√≥n A) - Implementado ‚úÖ  
**Producci√≥n**: RunPod Serverless (Opci√≥n C) - Documentado para futura implementaci√≥n

**Justificaci√≥n**:
- Desarrollo: Costo $0, latencia m√≠nima, control total
- Producci√≥n: Ahorro 76-97% en costos, auto-scaling, ideal para SaaS multi-tenant

### Pruebas Realizadas

#### Verificaci√≥n de Configuraci√≥n
```bash
# Verificar sintaxis de docker-compose.yml
docker compose config
```
‚úÖ Configuraci√≥n v√°lida

#### Modelo Actual
```bash
ollama list
```
‚úÖ Modelo `securetag-ai-agent:latest` (4.4 GB) confirmado

## üöß Cambios Implementados

*   [x] Investigaci√≥n de Opci√≥n A: Docker Local (Completado)
*   [x] Investigaci√≥n de Opci√≥n B: DigitalOcean GPU Droplets (Completado)
*   [x] Investigaci√≥n de Opci√≥n C: RunPod.io (Completado)
*   [x] An√°lisis comparativo con tabla de costos (Completado)
*   [x] Documento de investigaci√≥n (`LLM_Infrastructure_Research.md`) (Completado)
*   [x] Implementaci√≥n de servicio Ollama en `docker-compose.yml` (Completado)
*   [x] Configuraci√≥n de variables de entorno `OLLAMA_HOST` (Completado)
*   [x] Gu√≠a de setup (`Ollama_Setup_Guide.md`) (Completado)

## üìä Proyecci√≥n de Costos (Producci√≥n)

### Escenario: 100 an√°lisis/d√≠a
- **RunPod Serverless**: $54.25/mes
- **DigitalOcean GPU**: $2,255.70/mes
- **Ahorro**: $2,201.45/mes (97.6%)

### Escenario: 1,000 an√°lisis/d√≠a
- **RunPod Serverless**: $542.50/mes
- **DigitalOcean GPU**: $2,255.70/mes
- **Ahorro**: $1,713.20/mes (76%)

## üîÑ Pr√≥ximos Pasos

### Para Desarrollo (Inmediato)
1. Instalar `nvidia-container-toolkit` (si Linux/Windows WSL2)
2. Ejecutar `docker compose up -d ollama`
3. Cargar modelo usando gu√≠a en `docs/Ollama_Setup_Guide.md`
4. Verificar conectividad desde App/Worker

### Para Producci√≥n (Futuro)
1. Crear cuenta en RunPod.io
2. Subir modelo `securetag-ai-agent:latest`
3. Configurar endpoint Serverless
4. Implementar cliente API REST
5. Actualizar `OLLAMA_HOST` para apuntar a RunPod

## üí¨ Revisiones y comentarios del supervisor
*   **Veredicto**: ‚úÖ **Aprobado**
*   **Comentarios**:
    *   [x] Investigaci√≥n exhaustiva y muy valiosa. La elecci√≥n de RunPod para producci√≥n es acertada por costos y escalabilidad.
    *   [x] La implementaci√≥n local con Docker permite a los otros agentes (Worker/Fine-tuning) avanzar sin bloqueo.
    *   [x] **Nota**: El Agente Fine-tuning ya est√° utilizando esta investigaci√≥n para el entrenamiento del modelo. Buen trabajo transversal.
