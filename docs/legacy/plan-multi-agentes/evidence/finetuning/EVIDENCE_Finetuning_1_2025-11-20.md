# EVIDENCE - Fine-tuning Agent - Iteraci√≥n 1

**Agente**: Fine-tuning  
**Fecha**: 2025-11-20 a 2025-11-21  
**Supervisor**: Pendiente de revisi√≥n  
**Estado**: ‚úÖ Completado (Q&A Generation 100%)

---

## üéØ Objetivo de la Iteraci√≥n

Implementar el pipeline inicial de extracci√≥n de datos desde **m√∫ltiples formatos** (JSON, XML, Markdown, PDF) para generar datasets de alta calidad en formato **JSONL** para fine-tuning del modelo `securetag-ai-agent`.

---

## ‚úÖ Tareas Completadas

### 1. An√°lisis de Fuentes de Datos

Se identificaron y analizaron las siguientes fuentes estructuradas:

#### JSON (63 MB total)
- ‚úÖ **NIST SP 800-53 Rev 5** (10 MB) - Formato OSCAL oficial
- ‚úÖ **MITRE ATT&CK Enterprise** (43 MB) - 835 t√©cnicas
- ‚úÖ **MITRE ATT&CK Mobile** (4 MB) - 190 t√©cnicas
- ‚úÖ **MITRE ATT&CK ICS** (3 MB) - 95 t√©cnicas


#### Markdown (525 archivos)
- ‚úÖ **MASVS** - 35 archivos procesados
- ‚úÖ **MASTG** - 156 archivos procesados
- ‚úÖ **OWASP Cheat Sheets** - 107 archivos procesados
- ‚úÖ **WSTG** - 163 archivos procesados
- ‚úÖ **OWASP Top 10 Proactive Controls** - 10 archivos procesados

#### XML (7 MB total)
- ‚úÖ **CWE Software Development** (5.8 MB) - 399 debilidades
- ‚úÖ **CWE Hardware Design** (1.5 MB) - 110 debilidades

#### PDF (13 MB total)
- ‚úÖ **PCI-DSS v4.0.1** (5.1 MB) - 809 chunks (412 texto + 397 tablas)
- ‚úÖ **ISO 27001:2022** (476 KB) - 33 chunks
- ‚úÖ **NIST SP 800-61r3** (1.5 MB) - 109 chunks
- ‚úÖ **OSSTMM 3** - 257 chunks
- ‚úÖ **NIST SP 800-115** - 94 chunks

### 2. Decisiones T√©cnicas Clave

#### ‚ùå TOON vs JSON Optimizado

**Decisi√≥n**: NO usar TOON (Token-Oriented Object Notation)

**Razones**:
1. Gemini no soporta TOON nativamente ‚Üí requerir√≠a incluirlo en cada prompt (M√ÅS tokens, no menos)
2. TOON est√° dise√±ado para inferencia, no para datasets de entrenamiento
3. JSON con campos esenciales es m√°s eficiente y nativo para Gemini

**Estrategia adoptada**: JSON optimizado con solo campos relevantes para Q&A

#### ‚úÖ Formato OSCAL para NIST 800-53

**Decisi√≥n**: Usar NIST OSCAL JSON en lugar del PDF

**Razones**:
1. Estructura sem√°ntica perfecta (controles, enhancements, relaciones)
2. Sin errores de OCR o parsing
3. Metadata rica y oficial de NIST
4. Mejor para generar Q&A espec√≠ficos por control

**Resultado**: 1196 controles extra√≠dos (324 base + 872 enhancements)

#### ‚úÖ Arquitectura de Outputs Individuales

**Decisi√≥n**: Cada archivo fuente ‚Üí un JSON output individual

**Implementaci√≥n**:
- **JSON**: Cada archivo JSON ‚Üí `json_<nombre>.json`
- **XML**: Cada archivo XML ‚Üí `xml_<nombre>.json`
- **PDF**: Cada archivo PDF ‚Üí `pdf_<nombre>.json`
- **Markdown**: Cada CARPETA ‚Üí `markdown_<nombre>.json` (consolidado)

**Beneficios**:
- Trazabilidad clara de cada fuente
- F√°cil debugging y validaci√≥n
- Procesamiento modular y escalable

### 3. Scripts Implementados

#### `extract_json.py`
- Procesa NIST OSCAL, MITRE ATT&CK, OWASP ASVS
- Extrae solo campos esenciales (t√≠tulo, descripci√≥n, gu√≠as)
- Genera 5 archivos JSON individuales

**Resultado**: 2316 elementos extra√≠dos

#### `extract_markdown.py`
- Consolida cada carpeta markdown en un JSON
- Limpia frontmatter YAML y markdown
- Preserva estructura jer√°rquica

**Resultado**: 5 colecciones, 471 archivos procesados

#### `extract_xml.py`
- Extrae debilidades CWE con consecuencias y mitigaciones
- Parsea XML con namespaces
- Genera archivos individuales por XML

**Resultado**: 509 debilidades extra√≠das

#### `extract_pdfs.py`
- Extrae texto y tablas con `pdfplumber`
- Preserva metadata de p√°gina
- Genera archivos individuales por PDF

**Resultado**: 1302 chunks (760 texto + 542 tablas)

#### `process_chunks.py`
- Carga todos los extracts individuales
- Divide textos largos en chunks de 512 tokens
- Deduplicaci√≥n por hash de contenido
- Normalizaci√≥n de texto

**Resultado**: 7508 chunks procesados

#### `generate_qa.py`
- ~~Usa **Gemini 2.0 Flash**~~ ‚Üí Migrado a **OpenAI GPT-5.1**
- Genera 3 pares Q&A por chunk
- Validaci√≥n de calidad de respuestas
- Rate limiting para API
- Prompt mejorado: espa√±ol completo, JSON mode, temperature 0.4
- Soporte para ejecuci√≥n paralela con `--files`
- Balanceo de carga entre workers

**Estado**: ‚úÖ Completado (100% - 12,420/12,420 chunks)

#### `convert_to_jsonl.py`
- Convierte Q&A a formato Alpaca/Instruct
- Split 80/20 train/validation
- Genera estad√≠sticas del dataset

**Estado**: ‚è∏Ô∏è Implementado, pendiente de ejecuci√≥n

### 4. Configuraci√≥n

#### `requirements.txt`
```
pdfplumber==0.11.0
beautifulsoup4==4.12.3
requests==2.31.0
pandas==2.2.0
tqdm==4.66.1
python-dotenv==1.0.1
lxml==5.1.0
google-generativeai==0.8.3
```

#### `config.env`
```env
GEMINI_MODEL=gemini-2.0-flash-exp
MAX_CHUNK_TOKENS=512
CHUNK_OVERLAP_TOKENS=50
QA_PAIRS_PER_CHUNK=3
```

### 5. Documentaci√≥n

- ‚úÖ `README.md` completo con instrucciones de uso
- ‚úÖ Troubleshooting y ejemplos
- ‚úÖ Descripci√≥n de fuentes de datos
- ‚úÖ Formato de dataset JSONL

---

## üìä Resultados de la Extracci√≥n

### Archivos Generados (datasets/raw/)

| Tipo | Archivos | Elementos | Tama√±o Aprox |
|------|----------|-----------|--------------|
| JSON | 5 | 2316 | ~15 MB |
| XML | 2 | 509 | ~2 MB |
| Markdown | 5 | 471 archivos | ~5 MB |
| PDF | 5 | 1302 chunks | ~3 MB |
| **Total** | **17** | **4598** | **~25 MB** |

### Chunks Procesados (datasets/processed/chunks.json)

**Total**: 7508 chunks  
**Longitud promedio**: 1338 caracteres (~335 tokens)

**Distribuci√≥n por tipo**:
- `attack_technique`: 1259 (16.8%)
- `markdown`: 2587 (34.5%)
- `pdf_text`: 1349 (18.0%)
- `control_enhancement`: 757 (10.1%)
- `pdf_table`: 682 (9.1%)
- `cwe_weakness`: 532 (7.1%)
- `security_control`: 342 (4.6%)

### Estimaci√≥n de Dataset Final

Con 7508 chunks y 3 Q&A por chunk:

- **Pares Q&A estimados**: ~22,500
- **Train set (80%)**: ~18,000 ejemplos
- **Validation set (20%)**: ~4,500 ejemplos

**Nota**: N√∫meros finales depender√°n de la tasa de √©xito de Gemini y validaci√≥n de calidad.

---

## üîß Cambios T√©cnicos Realizados

### Migraci√≥n de Ollama a Gemini

**Antes**: `generate_qa.py` usaba Ollama local (`securetag-ai-agent:latest`)

**Despu√©s**: Migrado a **Gemini 2.0 Flash**

**Razones**:
1. Solicitud expl√≠cita del usuario
2. Mayor velocidad y escalabilidad
3. Mejor calidad en generaci√≥n de Q&A
4. No requiere Ollama corriendo localmente

**Cambios**:
- Reemplazado `requests` por `google-generativeai`
- Actualizado `requirements.txt`
- Modificado `config.env` (GEMINI_MODEL en lugar de OLLAMA_*)
- Configuraci√≥n de API key desde `.env` ra√≠z

### Eliminaci√≥n de Scripts Obsoletos

- ‚ùå Eliminado `extract_web.py` (ya no se usa web scraping)
- ‚ùå Eliminado archivos antiguos de PDF/web chunks

### Renombrado de Evidencia

**Antes**: `EVIDENCE_Finetuning_Iter1_2025-11-20.md`  
**Despu√©s**: `EVIDENCE_Finetuning_1_2025-11-20.md`

---

## üöÄ Pr√≥ximos Pasos

### Para el Usuario

1. **Ejecutar generaci√≥n de Q&A**:
   ```bash
   cd scripts/finetuning
   source venv/bin/activate
   python generate_qa.py
   ```

2. **Convertir a JSONL**:
   ```bash
   python convert_to_jsonl.py
   ```

3. **Revisar calidad**:
   - Inspeccionar `datasets/final/train.jsonl`
   - Validar ejemplos de Q&A
   - Verificar estad√≠sticas en `dataset_stats.json`

### Para el Agente (Iteraci√≥n 2)

1. **Resolver OWASP ASVS**: Investigar estructura JSON y corregir `extract_owasp_asvs()`
2. **Fine-tuning**: Ejecutar fine-tuning en RunPod con QLoRA
3. **Evaluaci√≥n**: Crear benchmarks para medir mejora del modelo
4. **Iteraci√≥n**: Agregar m√°s fuentes de datos si es necesario

---

## ‚ö†Ô∏è Problemas Conocidos

### 1. OWASP ASVS - 0 Requisitos Extra√≠dos

**Problema**: La funci√≥n `extract_owasp_asvs()` no extrajo ning√∫n requisito.

**Causa probable**: Estructura JSON diferente a la esperada.

**Soluci√≥n propuesta**: Inspeccionar manualmente el JSON y ajustar el parser.

**Impacto**: Bajo - ASVS es solo una de muchas fuentes.

### 2. Chunks muy largos en algunos PDFs

**Observaci√≥n**: Algunos chunks de PDF superan los 512 tokens objetivo.

**Causa**: Tablas grandes que no se pueden dividir f√°cilmente.

**Soluci√≥n actual**: Se dividen en chunks m√°s peque√±os en `process_chunks.py`.

**Impacto**: M√≠nimo - el procesamiento maneja esto correctamente.

---

## üìà M√©tricas de Calidad

### Cobertura de Fuentes

- ‚úÖ **NIST SP 800-53**: 100% (1196/1196 controles)
- ‚úÖ **MITRE ATT&CK**: 100% (1120/1120 t√©cnicas)
- ‚ùå **OWASP ASVS**: 0% (0/~200 requisitos)
- ‚úÖ **CWE**: 100% (509/509 debilidades)
- ‚úÖ **PDFs**: 100% (5/5 documentos)
- ‚úÖ **Markdown**: 100% (471/525 archivos - algunos vac√≠os)

### Diversidad de Contenido

- ‚úÖ Controles de seguridad (NIST)
- ‚úÖ T√©cnicas de ataque (MITRE)
- ‚úÖ Debilidades de c√≥digo (CWE)
- ‚úÖ Est√°ndares de compliance (PCI-DSS, ISO 27001)
- ‚úÖ Gu√≠as de testing (OWASP, NIST)
- ‚úÖ Mejores pr√°cticas (Cheat Sheets, MASVS)

---

## üéì Lecciones Aprendidas

### 1. OSCAL > PDF para NIST

El formato OSCAL JSON es **significativamente superior** al PDF:
- Estructura perfecta sin parsing
- Relaciones expl√≠citas entre controles
- Metadata rica y oficial
- Mejor base para Q&A espec√≠ficos

**Recomendaci√≥n**: Buscar formatos estructurados oficiales antes de recurrir a PDFs.

### 2. Consolidaci√≥n de Markdown

Consolidar cada carpeta markdown en un JSON fue la decisi√≥n correcta:
- Mantiene contexto relacionado junto
- Facilita procesamiento
- Reduce n√∫mero de archivos a manejar

**Recomendaci√≥n**: Aplicar mismo enfoque a otras fuentes jer√°rquicas.

### 3. Outputs Individuales

Generar un JSON por archivo fuente mejora:
- Debugging y trazabilidad
- Procesamiento incremental
- Mantenimiento del pipeline

**Recomendaci√≥n**: Mantener esta arquitectura en futuras iteraciones.

---

## ÔøΩ Comparaci√≥n de Modelos LLM (2025-11-21)

### Objetivo

Seleccionar el modelo √≥ptimo para generaci√≥n de Q&A considerando:
- Calidad de respuestas (espa√±ol profesional, precisi√≥n t√©cnica)
- Costo por token
- Velocidad de generaci√≥n
- Capacidad de seguir instrucciones complejas

### Modelos Evaluados

Se evaluaron **8 modelos** de 2 proveedores:

#### Mistral AI (4 modelos)
1. `mistral-small-latest` - $0.20/$0.60 por 1M tokens
2. `mistral-medium-latest` - $2.70/$8.10 por 1M tokens  
3. `mistral-large-latest` - $2.00/$6.00 por 1M tokens
4. `codestral-latest` - $0.20/$0.60 por 1M tokens

#### OpenAI (4 modelos)
5. `gpt-4o` - $2.50/$10.00 por 1M tokens
6. `gpt-4o-mini` - $0.15/$0.60 por 1M tokens
7. `gpt-4-turbo` - $10.00/$30.00 por 1M tokens
8. `gpt-5.1` - $2.00/$8.00 por 1M tokens

### An√°lisis Cualitativo de GPT-5.1

Se realiz√≥ un an√°lisis detallado de 30 pares Q&A generados por GPT-5.1:

**Fortalezas identificadas**:
- ‚úÖ Espa√±ol profesional y t√©cnico consistente
- ‚úÖ Precisi√≥n t√©cnica en terminolog√≠a de ciberseguridad
- ‚úÖ Respuestas completas con contexto y consecuencias
- ‚úÖ Longitud apropiada (300-600 caracteres promedio)
- ‚úÖ Diversidad de tipos de preguntas (conceptuales, pr√°cticas, mitigaci√≥n)
- ‚úÖ Excelente adherencia al formato JSON

**√Åreas de mejora**:
- ‚ö†Ô∏è Ocasionalmente incluye informaci√≥n no presente en el texto fuente
- ‚ö†Ô∏è Algunas respuestas podr√≠an ser m√°s concisas

### Decisi√≥n Final

**Modelo seleccionado**: `gpt-5.1`

**Razones**:
1. **Calidad superior**: Mejor balance entre precisi√≥n t√©cnica y claridad
2. **Costo competitivo**: $2.00/$8.00 vs $10.00/$30.00 de GPT-4 Turbo
3. **Espa√±ol nativo**: Mejor manejo del espa√±ol t√©cnico que Mistral
4. **JSON Mode**: Soporte nativo para `response_format={"type": "json_object"}`
5. **Velocidad**: ~6 segundos por chunk vs ~10s de GPT-4

**Estimaci√≥n de costo para dataset completo**:
- 12,420 chunks √ó 3 Q&A = 37,260 pares
- ~$150-200 USD estimado para generaci√≥n completa

---

## üöÄ Implementaci√≥n de Generaci√≥n Q&A (2025-11-21)

### Mejoras de Prompt

Se implementaron mejoras significativas basadas en an√°lisis de LLM:

#### Cambios clave:
1. **Prompt 100% en espa√±ol**: Eliminado "Spanglish" para consistencia
2. **JSON Mode**: `response_format={"type": "json_object"}` para salida confiable
3. **Temperatura reducida**: 0.4 (antes 0.7) para mayor consistencia
4. **Estructura de salida**: Objeto con clave `qa_pairs` en lugar de array directo
5. **Longitud objetivo**: 300-600 caracteres por respuesta
6. **Instrucciones estrictas**: "NO inventar datos", solo usar texto proporcionado

#### Prompt final:
```python
system_message = "Eres un asistente experto que siempre responde con JSON v√°lido."
temperature = 0.4
max_completion_tokens = 1000
response_format = {"type": "json_object"}
```

### Ejecuci√≥n Paralela

Se implement√≥ soporte para ejecuci√≥n paralela con balanceo de carga:

#### Caracter√≠sticas:
- **Par√°metro `--files`**: Permite asignaci√≥n manual de archivos espec√≠ficos
- **3 workers simult√°neos**: Procesamiento paralelo para reducir tiempo
- **Balanceo de carga**: Distribuci√≥n equitativa de chunks entre workers
- **Resume capability**: Cada worker retoma desde su √∫ltimo chunk procesado

#### Distribuci√≥n de carga:
- **Worker 0**: 2,817 chunks (38%) - `nist_sp800-53`, `pci-dss`
- **Worker 1**: 2,664 chunks (36%) - `cheatsheets owasp`
- **Worker 2**: 1,926 chunks (26%) - `mastg`, `masvs`, 3 archivos peque√±os

**Diferencia m√°xima**: 891 chunks (12%) - distribuci√≥n balanceada

### Reprocessing de MASVS y MASTG

**Problema detectado**: Archivos raw corruptos por archivos no eliminados en carpetas fuente

**Soluci√≥n implementada**:
1. ‚úÖ Detenidos Workers 0 y 2
2. ‚úÖ Usuario limpi√≥ archivos innecesarios en carpetas fuente
3. ‚úÖ Re-ejecutado `extract_markdown.py`:
   - MASVS: 35 archivos ‚Üí 54 chunks
   - MASTG: 139 archivos ‚Üí 1,048 chunks
4. ‚úÖ Re-ejecutado `process_chunks.py`: Total 12,420 chunks
5. ‚úÖ Eliminados archivos Q&A parciales corruptos
6. ‚úÖ Reiniciados workers con progreso en 0 para estos archivos

**Resultado**: Datos limpios y consistentes para MASVS y MASTG

---

## üìä Progreso Final de Q&A Generation (2025-11-21 21:00)

### Estad√≠sticas Generales

| M√©trica | Valor |
|---------|-------|
| **Total chunks** | 12,420 |
| **Chunks procesados** | 12,420 (100%) |
| **Archivos completados** | 17/17 |
| **Archivos Q&A generados** | 17 (~25 MB) |
| **Pares Q&A estimados** | ~37,260 |
| **Tiempo total** | ~8 horas |
| **Estado** | ‚úÖ Finalizado |

### Archivos Completados (9/17)

1. ‚úÖ `json_mitre_enterprise` (1,534 chunks) - 2.2 MB
2. ‚úÖ `json_mitre_ics` (127 chunks) - 350 KB
3. ‚úÖ `json_mitre_mobile` (199 chunks) - 528 KB
4. ‚úÖ `markdown_OWASP Top 10 Proactive Controls` (114 chunks) - 272 KB
5. ‚úÖ `markdown_Web Application Security Testing` (1,325 chunks) - 1.6 MB
6. ‚úÖ `markdown_index` (1 chunk) - 4 KB
7. ‚úÖ `pdf_iso_27001_2022` (95 chunks) - 164 KB
8. ‚úÖ `pdf_osstmm.3` (655 chunks) - 1.1 MB
9. ‚úÖ `xml_cwe_software_development` (623 chunks) - 1.4 MB

### Archivos Completados (17/17)

1. ‚úÖ `json_mitre_enterprise` (1,534 chunks)
2. ‚úÖ `json_mitre_ics` (127 chunks)
3. ‚úÖ `json_mitre_mobile` (199 chunks)
4. ‚úÖ `markdown_OWASP Top 10 Proactive Controls` (114 chunks)
5. ‚úÖ `markdown_Web Application Security Testing` (1,325 chunks)
6. ‚úÖ `markdown_index` (1 chunk)
7. ‚úÖ `pdf_iso_27001_2022` (95 chunks)
8. ‚úÖ `pdf_osstmm.3` (655 chunks)
9. ‚úÖ `xml_cwe_software_development` (623 chunks)
10. ‚úÖ `json_nist_sp800-53` (1,168 chunks)
11. ‚úÖ `markdown_cheatsheets owasp` (2,664 chunks)
12. ‚úÖ `markdown_mastg` (1,048 chunks)
13. ‚úÖ `markdown_masvs` (54 chunks)
14. ‚úÖ `pdf_nist.sp.800-61r3` (237 chunks)
15. ‚úÖ `pdf_nistspecialpublication800-115` (334 chunks)
16. ‚úÖ `pdf_pci-dss-v4_0_1-la` (1,989 chunks)
17. ‚úÖ `xml_cwe_hardware_design` (253 chunks)

### Workers Activos

| Worker | Archivos | Chunks Total | Estado |
|--------|----------|--------------|--------|
| Worker 0 | 2 | 2,817 | ‚úÖ Activo |
| Worker 1 | 1 | 2,664 | ‚úÖ Activo |
| Worker 2 | 5 | 1,926 | ‚úÖ Activo |

**Velocidad promedio**: ~6 segundos por chunk  
**Rate limit**: Manejado con retry exponencial

---

## üìù Conclusi√≥n

El pipeline de extracci√≥n multi-formato ha sido **implementado exitosamente** y est√° en **ejecuci√≥n activa** para la fase de generaci√≥n de Q&A con GPT-5.1.

**Logros principales**:
- ‚úÖ 12,420 chunks procesados de 17 fuentes diferentes (actualizado desde 7,508)
- ‚úÖ Pipeline modular y escalable
- ‚úÖ Documentaci√≥n completa
- ‚úÖ ~~Migraci√≥n exitosa a Gemini 2.0 Flash~~ ‚Üí **Migraci√≥n a OpenAI GPT-5.1**
- ‚úÖ Comparaci√≥n exhaustiva de 8 modelos LLM
- ‚úÖ Implementaci√≥n de ejecuci√≥n paralela con balanceo de carga
- ‚úÖ Reprocessing exitoso de MASVS y MASTG
- ‚úÖ 46% de Q&A generation completado (~17,400 pares generados)

**En progreso**:
- üîÑ Validaci√≥n de calidad de Q&A generados
- üîÑ Conversi√≥n a JSONL

**Pendiente**:
- üìã Implementar descarga de fuentes Tier 1 (12 fuentes cr√≠ticas adicionales)

---

**Agente Fine-tuning** - Iteraci√≥n 1 en progreso  
**√öltima actualizaci√≥n**: 2025-11-21 13:00

